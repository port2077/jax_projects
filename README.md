# JAX Learning Project

An exploration into the JAX ecosystem, comparing it with PyTorch through practical implementations. This was mostly sparked by the recent release of the JAX blog from Google:
https://cloud.google.com/blog/products/ai-machine-learning/guide-to-jax-for-pytorch-developers

The goal is to understand the key differences, performance characteristics, and practical considerations between these frameworks.

## Current Focus: MNIST Implementation

### Phase 1: Framework Comparison
- Implementing MNIST classification in PyTorch (baseline)
- Reimplementing the same model architecture in JAX
- Performance analysis:
  - Training time comparison
  - Memory usage
  - Code complexity differences
  - Key learning points about JAX's approach


## Future Plans

### Phase 2: Transformer Implementation
- Planning to implement a small-scale transformer (GPT-1 or GPT-2)
- Training on a modest text dataset
- Goals:
  - Better understand JAX's handling of more complex architectures
  - Explore JAX's optimization capabilities
  - Compare implementation complexity with PyTorch




